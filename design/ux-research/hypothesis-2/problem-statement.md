# Problem Statement: Coursework Marking & Data Drops

## ⚠️ Disclaimer

**This document contains hypothetical scenarios and opinions for educational and product development purposes only.**

- All scenarios, examples, and challenges described are entirely hypothetical and not based on any specific school or institution
- this document does not represent the views, experiences, or practices of any employer, school, or educational institution
- Information is based on publicly available research about UK secondary education in general
- No individual school, student, colleague, or workplace is being referenced or described
- This is personal research for product development, not professional advice
- All timings, quotes, and examples are illustrative and hypothetical

## The Teacher Scenario

**Context:** Secondary school Key Stage 3 (ages 11-14), 10 classes of 30 students each (90-330 students total), termly data drops requiring grades for every student.

**Current Workflow:**
1. Students complete coursework in class or at home (handwritten or typed)
2. Teacher collects work (physical books or digital submissions)
3. Teacher manually marks each piece (1-2 minutes per student)
4. Teacher records grades in spreadsheet/system
5. Teacher provides written feedback (WWW/EBI format)
6. Work returned to students (often days/weeks later)
7. Deadline pressure: Data drop submissions due to leadership

**Time Pressure:**
- Marking 330 students at 90 seconds average = 660 minutes 55 hours of marking
- Data drop deadlines: Often requires staying until 6pm+ to complete
- Feedback lag: Students receive feedback too late to be actionable
- Cognitive load: Maintaining consistency across hundreds of scripts

---

## Core Problem: Marking at Scale is Unsustainable

### The Pain Point

**"With 330 students to mark this term, I'm staying until 6pm to meet data drop deadlines."**

**What this means:**
- 55+ hours of marking for a single assessment (10 min � 330 students)
- Work-life balance suffers during data drop periods
- Feedback is delayed by 1-2 weeks (no longer actionable)
- Consistency issues when marking 330 scripts over several days
- Cognitive load of switching between rubric categories repeatedly
- Teacher burnout from repetitive marking tasks

### Why This Matters

**For the Teacher:**
- **Time burden:** Data drop deadlines force evening/weekend work
- **Consistency challenge:** Hard to maintain same standards across 330 scripts
- **Cognitive fatigue:** Repetitive evaluation drains mental energy
- **Delayed intervention:** By the time work is marked, students have moved on
- **Grade anxiety:** Pressure to complete data drops on time creates stress
- **Lost planning time:** Marking hours replace lesson preparation

**For Students:**
- **Delayed feedback:** Get marked work back 1-2 weeks later (too late to improve)
- **No immediate learning:** Miss opportunity for instant formative assessment
- **Limited feedback quality:** Rushed marking = brief comments
- **No peer comparison:** Can't see how their work compares to classmates
- **Passive learning:** Submit work � wait � receive grade (no active reflection)

---

## Current Solutions and Their Limitations

### Solution 1: Manual Marking (Paper-Based)

**What works:**
-  Familiar workflow (teachers know how to do this)
-  No tech dependency (works anywhere)
-  Authentic assessment format
-  Teacher control over feedback quality

**What doesn't work:**
- L **Extremely time-consuming** (55+ hours for 330 students)
- L Forces evening/weekend work to meet data drop deadlines
- L Consistency issues when marking over several days
- L Feedback delayed by 1-2 weeks (not actionable)
- L Cognitive fatigue from repetitive marking
- L No immediate formative feedback for students

**Teacher Quote:**
> "I've got 330 Year 9s to mark this term. I'll be here until 6pm for the next 4 days just to hit the data drop deadline."

---

### Solution 2: Digital Submission (Google Classroom)

**What works:**
-  Centralised collection (no lost notebooks)
-  Can mark anywhere (not tied to physical location)
-  Comment features for feedback
-  Automatic grade export

**What doesn't work:**
- L **Still requires manual marking** (time burden unchanged)
- L Same 55+ hour marking load
- L Doesn't solve the data drop deadline pressure
- L No AI assistance or automation
- L Students still wait 1-2 weeks for feedback

**Teacher Quote:**
> "Google Classroom helps me organise submissions, but I still have to read and mark every single one. It hasn't saved me any time."

---

### Solution 3: Self-Marking (Mark Scheme Provided)

**What works:**
-  Zero teacher marking time
-  Students engage with mark scheme
-  Immediate feedback (students mark in lesson)

**What doesn't work:**
- L **Students overmark themselves** (grade inflation)
- L Can't trust grades for official data drops
- L Students don't understand mark scheme nuances
- L No teacher verification = no accountability
- L Doesn't generate usable data for tracking/reporting

**Teacher Quote:**
> "Self-marking is great for formative practice, but I can't submit those grades to leadership. I need verified, consistent assessment."

---

### Solution 4: Peer Marking

**What works:**
-  Students learn from seeing others' work
-  Reduces teacher marking time (partially)
-  Encourages critical thinking

**What doesn't work:**
- L **Inconsistent standards** (students mark differently)
- L Can't use peer-awarded grades for data drops
- L Requires teacher moderation anyway (time not saved)
- L Students may not give honest feedback to friends
- L Doesn't solve the data drop deadline pressure

**Teacher Quote:**
> "Peer marking is useful for learning, but I still have to moderate all 330 pieces for the official grade. It doesn't actually save me time."

---

### Solution 5: Generic AI Tools (ChatGPT, Claude)

**What works:**
-  Can provide feedback on individual submissions
-  Fast turnaround (instant responses)
-  Can generate WWW/EBI comments

**What doesn't work:**
- L **Not designed for classroom workflow** (one-at-a-time process)
- L No bulk upload/batch processing
- L No mark scheme integration
- L No grade export for data drops
- L Teacher must copy-paste 330 times (prohibitive)
- L No consistency across responses (each prompt is isolated)

**Teacher Quote:**
> "I tried using ChatGPT to help with marking, but copying and pasting 330 student responses one by one would take longer than just marking them myself."

---

## The Real Competition: Manual Marking + Evening/Weekend Work

Let's be honest: the main alternative is **continuing to mark manually and accepting the time burden**.

**Why teachers stick with manual marking:**
- They know how to do it (familiar workflow)
- Leadership requires verified grades (can't trust self/peer marking)
- It works (eventually, with enough evening/weekend hours)
- No perceived alternative that meets data drop requirements

**What would make a teacher switch to AI-assisted marking:**
- Must **significantly reduce marking time** (from 55 hours to <10 hours)
- Must **maintain marking consistency** (not worse than human variability)
- Must **generate verified grades** acceptable for data drops
- Must **provide quality feedback** (WWW/EBI, not just a number)
- Must **preserve teacher control** (teacher can review/adjust AI suggestions)
- Must **work at scale** (300+ students without extra setup per student)

---

## Desired Outcomes

### For Teachers

**Primary:**
1. **Reduce marking time by 80%+**
   - From 55 hours to under 10 hours for 330 students
   - Meet data drop deadlines without evening/weekend work
   - Reclaim time for lesson planning and personal life

2. **Maintain marking consistency**
   - AI applies same rubric to all 330 students
   - Eliminate drift in standards over multi-day marking sessions
   - Reduce cognitive load of switching between criteria

3. **Generate verified grades for data drops**
   - Grades are accurate enough for official reporting
   - Teacher can review and adjust AI suggestions quickly
   - Export grades directly to school systems

**Secondary:**
4. Provide quality feedback (WWW/EBI) to students automatically
5. Identify class-wide patterns in student performance
6. Reduce cognitive fatigue from repetitive marking

### For Students

**Primary:**
1. **Receive instant feedback**
   - Get WWW/EBI comments immediately after submission
   - Learn from mistakes while content is fresh
   - Opportunity to revise/resubmit before final deadline

2. **Engage in self-assessment**
   - Reflect on own work before seeing AI feedback
   - Compare self-assessment to AI/peer/teacher feedback
   - Develop metacognitive skills (understand own learning)

**Secondary:**
3. See peer examples (if included in workflow)
4. Understand mark scheme through applied feedback
5. Receive more detailed feedback (AI doesn't fatigue like humans)

---

## Success Criteria

**The solution is working if:**

1. **Marking time reduced by 80%+**
   - 330 students marked in <10 hours (vs. 55 hours manual)
   - Teacher can meet data drop deadlines without overtime

2. **AI grades align with teacher judgement 85%+ of the time**
   - Teacher spot-checks 30 scripts: agrees with AI grade for 25+
   - Grade boundaries consistent with teacher's standards

3. **Teacher trusts the system enough to submit grades to leadership**
   - AI-assisted grades accepted for official data drops
   - Teacher feels confident defending grades if questioned

4. **Students receive feedback within 24 hours**
   - Faster than 1-2 week manual marking lag
   - Feedback still actionable (students can improve)

5. **Teacher uses it consistently (not one-off)**
   - Used for every data drop assessment
   - Becomes standard workflow (not a novelty)

**The solution is failing if:**

- AI grading is wildly inconsistent (teacher must re-mark everything)
- Setup/review time exceeds manual marking time (no time saved)
- Grades are inaccurate (leadership rejects AI-assisted grades)
- Teacher feels they're "fighting" the AI (frustration not relief)
- Students game the system (learn to trick AI rather than improve)

---

## Open Questions for Research

Before building, validate:

1. **What accuracy threshold do teachers need to trust AI grading?**
   - 70%? 85%? 95% alignment with teacher judgement?
   - How much review time is acceptable?

2. **Do schools/leadership accept AI-assisted grades for data drops?**
   - Is teacher verification sufficient?
   - Are there compliance/policy barriers?

3. **What rubric complexity can AI handle?**
   - Simple 1-5 scale? Multi-criteria rubrics?
   - Nuanced qualitative assessment or just quantitative?

4. **Does instant student feedback improve learning outcomes?**
   - Or is it "just" a time-saver for teachers?
   - Need evidence that students act on AI feedback

5. **Should self-assessment and peer review be mandatory or optional?**
   - Does multi-source feedback improve learning?
   - Or is it extra friction that reduces adoption?

6. **Would teachers pay for this?**
   - Is time savings worth a subscription cost?
   - What's the price threshold for adoption?

7. **What about academic integrity concerns?**
   - How to prevent students from submitting AI-generated work?
   - Does this create perverse incentives?

---

## Hypothesis to Test

**"If teachers could mark 330 students in under 4 hours with AI assistance while maintaining 85% grade accuracy, they would adopt this for all data drop assessments instead of manual marking."**

**How to test before building:**

1. **Wizard of Oz pilot** (3 teachers, 1 class each)
   - Students submit via Google Forms
   - You manually mark using consistent rubric (simulate AI)
   - Provide WWW/EBI feedback
   - Track time: How long to mark 30 students?
   - Teacher reviews: Do they trust the grades?
   - Ask: "Would you use this for data drops?"

2. **Interview 10 teachers across different subjects**
   - "How long does marking take for your last data drop?"
   - "What's most painful about the marking process?"
   - "What grade accuracy would you need to trust AI?"
   - "Would you submit AI-assisted grades to leadership?"
   - Listen for urgency ("I need this now") vs. politeness ("Nice idea")

3. **Prototype test with real AI**
   - Use Claude/ChatGPT API to mark 30 sample scripts
   - Compare AI grades to teacher grades (calculate accuracy)
   - Measure time: How long does teacher spend reviewing?
   - Total time: Is it faster than manual marking?

---

## The Real Pain vs. Nice-to-Have

**Real Pain (worth solving):**
- L 55+ hours marking for 330 students (teacher pain)
- L Evening/weekend work to meet data drop deadlines (teacher pain)
- L Cognitive fatigue from repetitive marking (teacher pain)
- L Consistency issues across 330 scripts (teacher pain)
- L Feedback delayed 1-2 weeks (student pain)
- L No formative feedback loop (student pain)

**Nice-to-Have (engagement, not pain):**
- Peer review features (learning benefit, but not core pain)
- Self-assessment (metacognitive value, but not essential)
- Heatmaps/analytics (useful, but not the primary driver)

**Critical Insight: Time Savings is the Core Value**

The product must **save teachers 45+ hours** to justify adoption.

- If it saves 45 hours but grades are 80% accurate � Teachers might adopt (time > perfection)
- If it's 99% accurate but saves 5 hours � Teachers won't switch (not worth learning new tool)

**Build for time savings first, accuracy second, features third.**

---

## Proposed Workflow

**Simple Teacher Workflow:**
1. Teacher creates assignment with mark scheme (5 categories)
2. Teacher shares URL with students (put on board/Google Classroom)
3. Students upload work (text or file upload)
4. [Optional] Students self-assess against mark scheme
5. [Optional] Students peer review (see anonymised peer work)
6. AI marks all submissions against mark scheme
7. AI provides WWW/EBI feedback to students
8. Teacher reviews all AI grades (spot-check, adjust outliers)
9. Teacher exports grades for data drop

**Time Comparison:**
- **Manual marking:** 55 hours for 330 students
- **AI-assisted:** 3-5 hours review + 2 hours setup = **7 hours total**
- **Time saved:** 48 hours (87% reduction)

---

## Critical Questions Before Building

1. **Can AI realistically mark open-ended coursework?**
   - Need to test with real examples (not just multiple choice)
   - What subjects/question types work best?

2. **Will teachers trust AI enough to submit grades to leadership?**
   - This is the adoption barrier
   - Without trust, tool is useless (can't solve data drop pain)

3. **Is 85% accuracy sufficient?**
   - Or does it need to be 95%+ to avoid teacher re-marking everything?
   - Need to test with real teachers

4. **Does self/peer review add value or just friction?**
   - Is the 4-way feedback (self/peer/AI/teacher) better for learning?
   - Or does it slow down the workflow without clear benefit?

5. **What's the minimum viable mark scheme?**
   - Simple 1-5 scale? Or multi-criteria rubric?
   - Start simple and test complexity later?

---

## Next Steps

1.  Document the problem (this file)
2. � Interview 10 teachers about marking workload and data drops
3. � Collect 30 sample student scripts with teacher marks
4. � Test AI marking accuracy (Claude API on sample scripts)
5. � Run Wizard of Oz test with 3 teachers (manual simulation)
6. � Decide: Is 80% time savings + 85% accuracy enough for adoption?
7. � Only then: Start building

---

## Iteration Strategy

### Micro-MVP (1-2 weeks):
- Text-only submission (no file upload yet)
- Simple 1-5 grade scale (not multi-criteria rubric)
- AI marking with WWW/EBI feedback
- Teacher review interface (adjust grades)
- Grade export (CSV for data drop)
- **Skip:** Self-assessment, peer review, analytics, fancy UI

**Test with 3 teachers, 1 class each:**
- Does it save time?
- Do teachers trust the grades?
- Would they use it for real data drops?

### Iteration 1 (1-2 months):
- Add file upload (PDF, DOCX, images)
- Multi-criteria rubric support (not just 1-5 scale)
- Self-assessment workflow (optional)
- Improved teacher review interface
- **Skip:** Peer review, advanced analytics

### Iteration 2 (2-3 months):
- Peer review workflow (optional)
- Class performance analytics (heatmaps)
- Integration with school MIS systems (SIMS, Arbor)
- Batch operations (mark 300 students at once)

---

## Appendix: Teacher's Own Experience

**As a teacher considering this tool:**
- What specific moment made you think "I need AI to help with marking"? *[The realisation that 330 Year 9 students need marking this term, and past experience of staying until 6pm during data drop periods]*
- How often do you face this marking burden? *[Every term for data drops, plus ad-hoc assessments throughout the year]*
- What would you do with the 45+ hours saved? *[Lesson planning, curriculum development, better work-life balance]*
- Would this tool have saved you time/stress last term? *[Absolutely - the time pressure around data drops is unsustainable]*
- If you had this tool tomorrow, would you use it? For what? *[Yes, for every data drop assessment and major coursework piece]*
- What accuracy would you need to trust it? *[85%+ alignment with my own judgement, with ability to quickly review and adjust outliers]*
