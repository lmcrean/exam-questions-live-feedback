  
# value hypothesis

## Teacher
- Spot patterns faster with collated student answers on whiteboard
- Objective data (Assessment heatmap)
- Can make data-driven decisionshello

## Student
- Spot patterns faster with collated student answers on whiteboard
- Pride/motivation from seeing their answer on the board 
- motivation from peer learning


# Wizard of Oz test

Set Exam Question on google classroom with Wooclap link
Review answers on Wooclap
- Allow voting.
- Chars are unlimited.
- This is very close to the Starter Hero idea, only without AI heatmap.

## Key Questions
- Do students care about seeing peer responses? Is it fun? Motivating?
- How much effort is it compared to Paper/ Verbal?
- Do they learn from multiple approaches?
- Is the manual aggregation too painful to sustain?
- Does voting identify quality answers or just popularity?
- Can I (teacher) spot patterns faster than paper method?

---

## Success Criteria

**Student Engagement (Primary):**
- ✅ Success: 80%+ students actively read at least 5 peer responses
- ✅ Success: Students ask "Can we do this again?"
- ❌ Failure: Students skim or skip peer responses entirely

**Student Learning (Primary):**
- ✅ Success: Evidence students revised understanding after seeing peers (e.g., "Oh I missed X" comments)
- ✅ Success: Class discussion references specific peer answers
- ❌ Failure: Students just vote for friends (popularity contest)

**Teacher Pattern-Spotting (Primary):**
- ✅ Success: I can identify common misconceptions faster than reading 30 notebooks
- ✅ Success: I spot patterns I wouldn't have caught with 5 verbal shares
- ❌ Failure: Still need to read all 30 responses manually (no time saved)

**Effort vs. Paper (Secondary):**
- ✅ Success: Total time ≤ 5 minutes more than paper workflow
- ❌ Failure: Setup + manual aggregation takes 10+ minutes extra

**Sustainability (Secondary):**
- ✅ Success: I'd be willing to do this every other lesson
- ❌ Failure: Too much manual effort, would revert to paper

---

## Measurement Plan

**Run this test for 2-3 lessons** (to get past novelty effect)

### Quantitative Data

**Timing:**
- [ ] Setup time (create Wooclap + share link)
- [ ] Submission phase duration
- [ ] Review phase duration
- [ ] Voting phase duration
- [ ] Total lesson time vs. paper baseline

**Participation:**
- [ ] Submission rate (X of 30 students submitted)
- [ ] Voting participation (X of 30 students voted)
- [ ] How many peer responses did students view? (estimate)

**Comparison to Paper:**
- [ ] Time saved/lost vs. paper workflow
- [ ] Number of student responses I actually reviewed

### Qualitative Data

**Student Reactions:**
- [ ] Direct quotes (excited? indifferent? frustrated?)
- [ ] Non-verbal cues (engaged? distracted?)
- [ ] Post-lesson: "Would you want to do this again?" (show of hands)

**Teacher Experience:**
- [ ] Could I spot patterns faster than paper?
- [ ] Did I identify misconceptions I would have missed?
- [ ] Stress level (easier or harder than paper?)
- [ ] Would I do this again tomorrow?

**Learning Evidence:**
- [ ] Did students revise understanding after peer review?
- [ ] Did voting identify quality answers or popularity?
- [ ] Did class discussion improve (based on actual work vs. hypotheticals)?

---

## Decision Tree (After 2-3 Tests)

### Scenario A: Students Love It, I Love It ✅
**Evidence:**
- Students engage with peer responses
- Voting identifies strong answers (not popularity)
- I can spot patterns faster than paper
- Setup effort is acceptable (≤5 min overhead)

**→ Next Action: Build Iteration 1**
- Automate the Wooclap workflow
- Add mark scheme integration
- Add stage-based transitions
- Timeline: 1-2 months to working prototype

---

### Scenario B: Students Love It, I Struggle ⚠️
**Evidence:**
- Students engage and learn from peers
- But I can't spot patterns fast enough (too many responses)
- Manual aggregation is painful
- Takes too long to identify misconceptions

**→ Next Action: Build Iteration 3 First (AI Heatmap)**
- Skip peer voting features initially
- Focus on AI analysis + heatmap for teacher
- Add peer features later if time permits
- Timeline: 2-4 months (AI complexity)

---

### Scenario C: Students Indifferent, Voting Fails ⚠️
**Evidence:**
- Students skim peer responses (don't engage deeply)
- Voting becomes popularity contest
- No evidence of peer learning
- But I (teacher) find value in seeing all responses

**→ Next Action: Pivot to AI-First Product**
- Drop peer voting/engagement features
- Focus on teacher pain (pattern-spotting)
- Build: Submit → AI analyze → Heatmap
- Skip: Voting, leaderboards, peer review

---

### Scenario D: Total Failure ❌
**Evidence:**
- Too much friction (tech issues, setup time)
- Students prefer paper (more authentic practice)
- I prefer paper (faster, more reliable)
- No clear benefit over current workflow

**→ Next Action: Kill the Project**
- Accept that paper works better for this use case
- Saved 3-6 months of building something nobody wants
- Move on to different problem

---

## Test Protocol (For Consistency)

### Before Lesson 1:
- [ ] Create Wooclap with exam question
- [ ] Share link via Google Classroom
- [ ] Set expectations with students ("We're trying something new")

### During Lesson (Repeat for 2-3 lessons):
1. **Submission Phase (10 min)**
   - Students answer via Wooclap
   - Track: submission rate, time taken

2. **Review Phase (5 min)**
   - Display all responses on board (Wooclap interface)
   - Students read peer answers
   - Track: engagement (are they reading or waiting?)

3. **Voting Phase (3 min)**
   - Students vote for top 3 answers
   - Track: voting participation, vote patterns

4. **Discussion Phase (5 min)**
   - Discuss top-voted answers
   - Teacher highlights patterns/misconceptions
   - Track: quality of discussion vs. paper method

### After Lesson:
- [ ] Student quick poll: "Was this helpful? Would you do it again?"
- [ ] Teacher notes: What worked? What didn't? Would I do this again?
- [ ] Compare to paper baseline: Time? Engagement? Learning?

---

## Test Results (To Be Filled In)

### Lesson 1: [Date]
**Topic:** [Exam question topic]
**Submission rate:** X/30
**Voting participation:** X/30
**Time:** Setup X min, Total X min vs. paper X min
**Student reaction:** [Quotes/observations]
**Teacher notes:** [Pattern-spotting success? Would do again?]

### Lesson 2: [Date]
**Topic:** [Exam question topic]
**Submission rate:** X/30
**Voting participation:** X/30
**Time:** Setup X min, Total X min vs. paper X min
**Student reaction:** [Quotes/observations]
**Teacher notes:** [Pattern-spotting success? Would do again?]

### Lesson 3: [Date]
**Topic:** [Exam question topic]
**Submission rate:** X/30
**Voting participation:** X/30
**Time:** Setup X min, Total X min vs. paper X min
**Student reaction:** [Quotes/observations]
**Teacher notes:** [Pattern-spotting success? Would do again?]

---

## Final Decision (After All Tests)

**Which scenario did we hit?** [A / B / C / D]

**Evidence:** [Key data points that led to this conclusion]

**Next Action:** [Build Iteration 1 / Build Iteration 3 / Pivot / Kill]

**Timeline:** [When will I start building?]